<!DOCTYPE html>
<!--
    Plain-Academic by Vasilios Mavroudis
    Released under the Simplified BSD License/FreeBSD (2-clause) License.
    https://github.com/mavroudisv/plain-academic
-->

<html lang="en">
<head>
  <title>Shadan Golestan</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.0/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js"></script>
  <link href='https://fonts.googleapis.com/css?family=Oswald:700' rel='stylesheet' type='text/css'>
</head>
<body>


<!-- Navigation -->
	<nav class="navbar navbar-inverse navbar-static-top" role="navigation">
	  <div class="container">
		<div class="navbar-header">
		  <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
						<span class="sr-only">Toggle navigation</span>
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
						<span class="icon-bar"></span>
					</button>
		</div>

		<!-- Collect the nav links, forms, and other content for toggling -->
		<div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
		  <ul class="nav navbar-nav">
        <li><a href="index.html">Home</a></li>
        <li><a style="color:red" href="Research.html">Research</a></li>
        <li><a href="Experience.html">Experience</a></li> 
        <li><a href="Publications.html">Publications</a></li>
        <li><a href="Professional_Service.html">Professional Service</a></li>
        <li><a href="Teaching.html">Teaching</a></li>
        <li><a href="#news">News</a></li> 
        <li><a target="_blank" href="Files/My__CV.pdf">CV</a></li> 
        <li><a href="Resources.html">Resources</a></li> 
        <li><a href="pics.html">Others</a></li> 
		  </ul>
		</div>
	  </div>
	</nav>
  
  <!-- Page Content -->
    <div class="container">

        <div class="row">

            <!-- Entries Column -->
            <div class="col-md-8" style="height: 100vh;">
                
                <!-- Main Image -->
                <div style="font-family: 'Oswald', sans-serif; font-size: 32px;"><b>Research</b></div><br>
                
                <div style="margin-top:3%; text-align:justify;">                
                
                  <h3 style="font-size: 20px;"> Sequential Decision Making via Reinforcement Learning, Generative Models and LLMs/VLMs:</h3>

                  <ul>
                    <li class="paper"> <strong>The Evolving Landscape of LLM-and VLM-Integrated Reinforcement Learning</strong> 
                      <dd>
                        <a target="_blank" href="https://arxiv.org/abs/2502.15214">Paper</a>
                        |
                        <a target="_blank" href="">Code</a>
                      </dd> <br>
                    <p>
                      Reinforcement learning (RL) has shown impressive results in sequential decision-making tasks, while Large Language Models (LLMs) and Vision-Language Models (VLMs) have emerged with remarkable capabilities in multimodal understanding and reasoning. This survey reviews representative works where LLMs and VLMs are used to overcome key challenges in RL, such as lack of prior knowledge, long-horizon planning, and reward design. We present a taxonomy categorizing these approaches into three roles: agent, planner, and reward. The survey establishes a framework for integrating LLMs and VLMs into RL, advancing approaches that unify natural language and visual understanding with sequential decision-making.
                    </p>
                    <p>
                      <div style="display: flex; justify-content: center;">
                        <img class="img-responsive" src="Research pictures/Survey/image.png" alt="" width="500" height="500">
                    </div>
                    </p>
                  </ul>

                  <ul>
                    <li class="paper"> <strong>A Study of the Efficacy of Generative Flow Networks for Robotics and Machine Fault-Adaptation</strong> 
                      <dd>
                        <a target="_blank" href="https://arxiv.org/abs/2407.15283">Paper</a>
                        |
                        <a target="_blank" href="">Code</a>
                      </dd> <br>
                    <p>
                      Advancements in robotics enable automation across manufacturing, emergency response, and healthcare. However, robots face challenges in out-of-distribution (OOD) situations, particularly when encountering hardware faults. Current reinforcement learning algorithms show promise but suffer from sample inefficiency and slow adaptation. Our research investigates the efficacy of generative flow networks (CFlowNets) for machine fault adaptation in robotic environments. Using a modified Reacher environment with four distinct fault scenarios, we demonstrate that CFlowNets can add adaptive behaviors under adversarial conditions. Comparative analysis with RL algorithms reveals insights into adaptation speed and sample efficiency. Our experiments confirm CFlowNets' potential for real-world deployment, showing adaptability to maintain functionality during malfunctions.
                    </p>
                    <p>
                      <div style="display: flex; justify-content: center;">
                        <img class="img-responsive" src="Research pictures/CFlowNets/image.png" alt="" width="500" height="500">
                    </div>
                    </p>
                  </ul>

                  <ul>
                    <li class="paper"> <strong>Adaptive Iterative Feedback Prompting for Obstacle-Aware Path Planning via LLMs</strong> 
                      <dd>
                        <a target="_blank" href="">Paper</a>
                        |
                        <a target="_blank" href="">Code</a>
                      </dd> <br>
                    <p>
                      Planning is crucial for agents in complex decision-making tasks, especially in Human-Robot Interaction (HRI) scenarios requiring adaptability in dynamic environments. Large Language Models (LLMs) show promise for enhancing planning through natural language understanding, but their effectiveness is limited by spatial reasoning shortcomings. Existing LLM-based planning frameworks often rely on classical methods or struggle with dynamic environments. We propose the "Adaptive Iterative Feedback Prompting" (AIFP) framework for path planning, where an LLM generates partial trajectories iteratively, evaluated for potential collisions using environmental feedback. AIFP either executes the trajectory or re-plans based on evaluation results. Our preliminary results show AIFP increases baseline success rate by 33.3% and generates efficient, appropriately complex paths, making it promising for dynamic HRI scenarios.
                    </p>
                    <p>
                      <div style="display: flex; justify-content: center;">
                        <img class="img-responsive" src="Research pictures/AIFP/image.png" alt="" width="500" height="500">
                    </div>
                    </p>
                  </ul>

                  <ul>
                    <li class="paper"> <strong>Enhancing Hardware Fault Tolerance in Autonomous Machines Using Reinforcement Learning Policy Gradient Algorithms</strong> 
                      <dd>
                        <a target="_blank" href="https://arxiv.org/pdf/2407.15283">Paper</a>
                        |
                        <a target="_blank" href="">Code</a>
                      </dd> <br>
                    <p>
                      The industry is shifting towards autonomous systems capable of detecting and adapting to machine hardware faults. 
                      Traditional fault tolerance involves duplicating components and reconfiguring processes, 
                      but reinforcement learning (RL) offers a new approach. 
                      This paper explores the potential of two RL algorithms, PPO and SAC, 
                      for enhancing hardware fault tolerance in machines. 
                      Tested in OpenAI Gym environments (Ant-v2, FetchReach-v1) with six simulated faults, 
                      results show RL enables rapid adaptation. 
                      PPO adapts best by retaining knowledge, while SAC performs better when discarding it, 
                      highlighting RL's potential for developing robust, adaptive machines.
                    </p>
                    <p>
                      <div style="display: flex; justify-content: center;">
                        <img class="img-responsive" src="Research pictures/FaultAdaptation.png" alt="" width="500" height="500">
                    </div>
                    </p>
                  </ul>


                  <ul>
                    <li class="paper"> <strong>Explainability of Deep Reinforcement Learning Algorithms in Robotics</strong> 
                      <dd>
                        <a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0952197624012892">Paper</a>
                        |
                        <a target="_blank" href="https://github.com/MehranTaghian/SAC_GCN">Code</a>
                      </dd> <br>
                    <p>
                      Deep reinforcement learning (DRL) has been successful in robotics but lacks explainability. 
                      This research proposes using Graph Networks and Layer-wise Relevance Propagation (LRP) to analyze the learned representations of a robot's observations. 
                      By representing observations as entity-relationship graphs, 
                      we can interpret the robot's decision-making process, 
                      compare different policies, 
                      and understand how the robot recovers from errors. 
                      This approach contributes to making DRL in robotics more transparent and understandable.
                    </p>
                    <p>
                      <div style="display: flex; justify-content: center;">
                      <img class="img-responsive"  src="Research pictures/explainability.jpeg" alt=""  width="500" height="500"><br>
                    </div>
                    </p>
                  </ul>



                  <h3 style="font-size: 20px;">Bayesian Optimization:</h3>

                  <ul>
                    <li class="paper"> <strong>Grey-box Bayesian Optimization</strong> 
                      <dd>
                        <a target="_blank" href="https://arxiv.org/abs/2309.05784">Paper</a>
                        |
                        <a target="_blank" href="https://github.com/shadangolestan/SensorConfigOptimization">Code</a>
                        |
                        <a target="_blank" href="Files/AAAI_Slides.pdf">Slides</a>
                      </dd> <br>
                    <p>
                      We propose novel grey-box Bayesian optimization algorithms, which focuses on learning domain-specific knowledge inherent to the function being optimized. 
                      This leads to designing acquisition functions that incorporate the knowledge into the decision-making process for choosing the next candidate point.
                      In the example provided below, step 1 calculates a credit for each parameter of the candidate point (brighter colors mean more credit).
                      We construct an information profiles that is used for calculating the expected information gain of each parameter in step 2.
                      In step 2, we use the previous version of the expected information gain of the best answer that we have found so far, x_star.
                      The expected informatio gain then tells us the next query point.
                    </p>
                    <p>
                      <div style="display: flex; justify-content: center;">
                      <img class="img-responsive"  src="Research pictures/grey_box.png" alt=""  width="500" height="500"><br>
                    </div>
                    </p>
                  </ul>

                  <ul>
                    <li class="paper"> <strong>Simulation-driven Sensor Placement Optimization Framework</strong> 
                      <dd>
                        <a target="_blank" href="https://era.library.ualberta.ca/items/5dbd68c0-78e2-472c-a100-8751b1117db3">PhD Dissertation</a>
                        |
                        <a target="_blank" href="https://github.com/shadangolestan/SensorConfigOptimization">Code</a>
                      </dd> <br>
                    <p>
                      We propose a framework that at its heart, lies an optimization algorithm.
                      The algorithm iteratively observes the previous outcome of a sensor placement and 
                      accordingly proposes the next one.
                      The candidate sensor placement goes to a simulation software that produces a synthetic, but realistic
                      datasets according to the occupants daily activity plans, indoor space layout and sensors. 
                      The dataset consists of occupants activities and corresponding sensor readings.
                      This dataset is then used by an activity classifier to train and test an activity recognition model.
                      The performance of the activity recognition model is reported as the quality of the candidate sensor placement.

                    </p>
                    <p>
                      <div style="display: flex; justify-content: center;">
                      <img class="img-responsive"  src="Research pictures/simulation.png" alt=""  width="500" height="500"><br>
                    </div>
                    </p>
                  </ul>

                  <h3 style="font-size: 20px;">Simulation Methodologies:</h3>

                  <ul>
                    <li class="paper"> <strong>Sim SIS: A Simulation Framework for Smart Indoor Spaces</strong> 
                      <dd>
                        <a target="_blank" href="https://www.mdpi.com/1424-8220/20/24/7137">Paper</a>
                        |
                        <a target="_blank" href="https://github.com/shadangolestan/SIM_SIS-Simulator">Code</a>
                      </dd> <br>
                    <p>
                      Optimizing sensor placement in smart homes and buildings is challenging due to the time and cost of real-world testing. 
                      This research presents a simulation tool called SIMsis that models indoor spaces, occupant activities, and sensor behaviors. 
                      SIMsis generates realistic sensor data over time, 
                      which can be used to evaluate different sensor configurations without physical experimentation. 
                      We tested SIMsis in a smart home setting (Real-World Measurements) and found it effectively simulates real-world conditions, making it a valuable tool for developing and deploying sensor-based applications.

                    </p>
                    <p>
                      <div style="display: flex; justify-content: center;">
                      <img class="img-responsive"  src="Research pictures/simsis.png" alt=""  width="700" height="700"><br>
                    </div>
                    </p>
                  </ul>

                  
                
					
                </div>
            </div> 

<!-- Contact Info on the Sidebar -->
<div class="col-md-4">
  <div style="font-family: 'Oswald', sans-serif; font-size: 32px;"><b>Shadan Golestan</b></div><br>
  <div style="display: flex; align-items: center; margin-bottom: 10px;">
    <img src="logos/University of Alberta.png" alt="University Logo" style="max-width: 100px; height: auto; margin-right: 5px;">
    <img src="logos/amii_logo_new.png" alt="Amii Logo" style="max-width: 100px; height: auto;">
</div>
<p><b>golestan@ualberta.ca</b>
  <b>golestan@amii.ca</b><br></p>
    Machine Learning Scientist<br>
    Alberta Machine Intelligence Institute (Amii)<br>
  </p>
</div>

      

      <!-- Links on the Sidebar -->
      <div class="col-md-4">
          <div style="margin-top: 2%">
              <dd><a target="_blank" href="https://scholar.google.ca/citations?user=JV9e-TkAAAAJ&hl=en&authuser=2">Google Scholar</a></dd>
              <dd><a target="_blank" href="https://github.com/shadangolestan">GitHub</a></dd>
              <dd><a target="_blank" href="https://twitter.com/shgolestan">Twitter</a></dd>
              <dd><a target="_blank" href="https://www.linkedin.com/in/shgolestan/">LinkedIn</a></dd>
          </div>
      </div>


    </div>
    <!-- /.container -->
    
    
</body>

</html>
